{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "220fab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.utils import tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import fasttext\n",
    "from timeit import default_timer as timer\n",
    "from time import strftime as time\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedb0c1f",
   "metadata": {},
   "source": [
    "### Load Fasttext vectors\n",
    "Pretrained Fasttext-Embeddings (common crawl) over 300 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdd65aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_standard_model():\n",
    "    fasttext_file = datapath(\"C:/Users/flohk/OneDrive/Uni/Projektseminar/data/crawl-300d-2M-subword/crawl-300d-2M-subword.bin\")\n",
    "    return fasttext.load_facebook_model(fasttext_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e592a5",
   "metadata": {},
   "source": [
    "### Load twitter data\n",
    "Data from https://github.com/GT-SALT/implicit-hate\n",
    "Any explicit hate is filtered out so that there is only non-explicit hate and non-explicit non-hate left. It might be a good idea to continue training the model on all of the data, but for these purposes, we want to keep everything in-domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb8a4fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/implicit-hate-corpus/implicit_hate_v1_stg1_posts.tsv\", delimiter=\"\\t\")\n",
    "df = df[df[\"class\"] != \"explicit_hate\"]\n",
    "sents = [list(tokenize(item)) for item in list(df[\"post\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebd5632e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310440"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(itertools.chain.from_iterable(sents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc84f070",
   "metadata": {},
   "source": [
    "Now, fasttext can be finetuned by reading in these additional lines. All the hyper-parameters will be left as is so that finetuning is performed in accordance with how the model is already trained (the standard hyperparameters seems to match the ones used by facebook in https://arxiv.org/pdf/1712.09405.pdf, namely a window size of 5 and usage of cbow instead of skipgram). However, the amount of epochs need to be accounted for.\n",
    "\n",
    "Ideally, a grid-search of all relevant hyper-parameters (larning rate, window size and learning algorithm), each configuration being used to train and evaluate a classifier, would be performed. However, that would take anywhere from 15 minutes to over 5 hours on my machine and thus, only amount of epochs will be varied. \n",
    "\n",
    "Six different models will be created and later tested against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "016a2105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(model, n_epochs):\n",
    "    model.build_vocab(corpus_iterable=sents, update=True)\n",
    "    model.train(sents, total_examples=len(sents), epochs=n_epochs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db323349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, name):\n",
    "    \"\"\"\n",
    "    # Old code for creating model names, might use again later\n",
    "    models = [m for m in os.listdir(\"C:/Users/flohk/OneDrive/Uni/Projektseminar/data/fasttext-finetune/\") if m.endswith(\".model\")]\n",
    "    max_num = -1\n",
    "    for m in models:\n",
    "        try:\n",
    "            max_num = max(int(re.search(r'model(\\d+).model', m).group(1)), max_num)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    max_num += 1\n",
    "    \"\"\"\n",
    "    model.save(get_tmpfile(\"C:/Users/flohk/OneDrive/Uni/Projektseminar/data/fasttext-finetune/model\" + name + \".model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fd2d08",
   "metadata": {},
   "source": [
    "Just to make sure that finetuning actually causes a significant difference in the vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c3b6bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hate: False 0.348940372467041\n",
      "christ: False 0.447144091129303\n",
      "black: False 0.3751109838485718\n",
      "cat: False 0.2610793709754944\n",
      "spruce: False 0.3200603127479553\n"
     ]
    }
   ],
   "source": [
    "test_model = load_standard_model()\n",
    "\n",
    "hate_old = np.copy(test_model.wv['hate'])\n",
    "christ_old = np.copy(test_model.wv['christ'])\n",
    "black_old = np.copy(test_model.wv['black'])\n",
    "cat_old = np.copy(test_model.wv['cat'])\n",
    "spruce_old = np.copy(test_model.wv['spruce'])\n",
    "\n",
    "test_model = finetune(test_model, 5)\n",
    "\n",
    "hate_new = np.copy(test_model.wv['hate'])\n",
    "christ_new = np.copy(test_model.wv['christ'])\n",
    "black_new = np.copy(test_model.wv['black'])\n",
    "cat_new = np.copy(test_model.wv['cat'])\n",
    "spruce_new = np.copy(test_model.wv['spruce'])\n",
    "\n",
    "print(\"hate:\", np.allclose(hate_old, hate_new, atol=1e-4), cosine(hate_old, hate_new))\n",
    "print(\"christ:\", np.allclose(christ_old, christ_new, atol=1e-4), cosine(christ_old, christ_new))\n",
    "print(\"black:\", np.allclose(black_old, christ_new, atol=1e-4), cosine(black_old, black_new))\n",
    "print(\"cat:\", np.allclose(cat_old, cat_new, atol=1e-4), cosine(cat_old, cat_new))\n",
    "print(\"spruce:\", np.allclose(spruce_old, spruce_new, atol=1e-4), cosine(spruce_old, spruce_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369f46c7",
   "metadata": {},
   "source": [
    "There is quite a significant difference after finetuning on only 10 epochs in words that are more strongly related to hatred and even (presumably) unrelated words, so anything more might be hardcore overfitting. However, still going to do it, just because. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9458850a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training at 05:28.41\n",
      "Completed epoch 1 at 05:31.48\n",
      "Completed epoch 5 at 05:35.04\n",
      "Completed epoch 25 at 05:38.32\n",
      "Completed epoch 150 at 05:43.19\n",
      "Completed epoch 625 at 05:52.06\n",
      "Completed epoch 3125 at 06:22.31\n"
     ]
    }
   ],
   "source": [
    "all_epochs = [1, 5, 25, 150, 625, 3125]\n",
    "print(\"Started training at\", time(\"%H:%M.%S\"))\n",
    "for epoch in all_epochs:\n",
    "    save_model(finetune(load_standard_model(), epoch), str(epoch))\n",
    "    print(\"Completed epoch\", epoch, \"at\", time(\"%H:%M.%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7c987e",
   "metadata": {},
   "source": [
    "# Testing the implementations\n",
    "For all embeddings, a model is trained with the parameters and hyperparameters previously found out. A 5 fold cross-validation is performed to evaluate the performance of each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "096c539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "from collections import Counter\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b42a410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>class</th>\n",
       "      <th>ID</th>\n",
       "      <th>implicit_class</th>\n",
       "      <th>extra_implicit_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\" : jewish harvard professor noel ignatiev w...</td>\n",
       "      <td>implicit_hate</td>\n",
       "      <td>399886440588247041</td>\n",
       "      <td>white_grievance</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b.higher education is a part of european cult...</td>\n",
       "      <td>not_hate</td>\n",
       "      <td>929901925100937216</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>has a problem with  \" the whites \" \" and \" \" ...</td>\n",
       "      <td>not_hate</td>\n",
       "      <td>728678509497954304</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is yasir qadhi a hate preacher for calling ch...</td>\n",
       "      <td>not_hate</td>\n",
       "      <td>625688620444180481</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rt  \" : how three million germans mass murder...</td>\n",
       "      <td>not_hate</td>\n",
       "      <td>441089979322597376</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20386</th>\n",
       "      <td>Seeing prostitutes is morally wrong, but being...</td>\n",
       "      <td>implicit_hate</td>\n",
       "      <td>SAP_17154</td>\n",
       "      <td>irony</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20387</th>\n",
       "      <td>I wonder how many females I raped today I spen...</td>\n",
       "      <td>implicit_hate</td>\n",
       "      <td>SAP_17179</td>\n",
       "      <td>irony</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20388</th>\n",
       "      <td>Having a criminal record is more attractive to...</td>\n",
       "      <td>implicit_hate</td>\n",
       "      <td>SAP_17314</td>\n",
       "      <td>stereotypical</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20389</th>\n",
       "      <td>Another advantage of being a female: getting b...</td>\n",
       "      <td>implicit_hate</td>\n",
       "      <td>SAP_17343</td>\n",
       "      <td>inferiority</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20390</th>\n",
       "      <td>If men are so privileged and women have it so ...</td>\n",
       "      <td>implicit_hate</td>\n",
       "      <td>SAP_17406</td>\n",
       "      <td>irony</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20391 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    post          class  \\\n",
       "0        \" : jewish harvard professor noel ignatiev w...  implicit_hate   \n",
       "1       b.higher education is a part of european cult...       not_hate   \n",
       "2       has a problem with  \" the whites \" \" and \" \" ...       not_hate   \n",
       "3       is yasir qadhi a hate preacher for calling ch...       not_hate   \n",
       "4       rt  \" : how three million germans mass murder...       not_hate   \n",
       "...                                                  ...            ...   \n",
       "20386  Seeing prostitutes is morally wrong, but being...  implicit_hate   \n",
       "20387  I wonder how many females I raped today I spen...  implicit_hate   \n",
       "20388  Having a criminal record is more attractive to...  implicit_hate   \n",
       "20389  Another advantage of being a female: getting b...  implicit_hate   \n",
       "20390  If men are so privileged and women have it so ...  implicit_hate   \n",
       "\n",
       "                       ID   implicit_class extra_implicit_class  \n",
       "0      399886440588247041  white_grievance                  NaN  \n",
       "1      929901925100937216              NaN                  NaN  \n",
       "2      728678509497954304              NaN                  NaN  \n",
       "3      625688620444180481              NaN                  NaN  \n",
       "4      441089979322597376              NaN                  NaN  \n",
       "...                   ...              ...                  ...  \n",
       "20386           SAP_17154            irony                  NaN  \n",
       "20387           SAP_17179            irony                  NaN  \n",
       "20388           SAP_17314    stereotypical                  NaN  \n",
       "20389           SAP_17343      inferiority                  NaN  \n",
       "20390           SAP_17406            irony                  NaN  \n",
       "\n",
       "[20391 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/implicit-hate-corpus/implicit_hate_v1_stg1_posts.tsv\", delimiter=\"\\t\")\n",
    "df_id = pd.read_csv(\"data/implicit-hate-corpus/implicit_hate_v1_stg1.tsv\", delimiter=\"\\t\")\n",
    "df[\"ID\"] = df_id[\"ID\"]\n",
    "df = df[df[\"class\"] != \"explicit_hate\"]\n",
    "df_class = pd.read_csv(\"data/implicit-hate-corpus/implicit_hate_v1_stg2.tsv\", delimiter=\"\\t\")\n",
    "df = pd.merge(df, df_class, on=\"ID\", how=\"left\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6920c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "stemmer = PorterStemmer()\n",
    "stop = stopwords.words(\"English\")\n",
    "caps_tokens = re.compile(r\"\\w+(?:'\\w+)?|[^\\w\\s]\")\n",
    "\n",
    "def vectorize(sent, clean_input=['\"'], features=[\"word2vec\"], model=None):\n",
    "    \n",
    "    # Remove unwanted substrings from input\n",
    "    for subs in clean_input:\n",
    "        sent = sent.replace(subs, \"\")\n",
    "    \n",
    "    feature_vector = []\n",
    "    \n",
    "    # Fasttext vectors\n",
    "    if \"fasttext\" in features:\n",
    "        tokens = [token for token in tokenizer.tokenize(sent) if token not in stop]\n",
    "        vecs = np.zeros(300)\n",
    "        for token in tokens:\n",
    "            vecs += model[token]\n",
    "        if len(tokens) != 0:\n",
    "            vecs = vecs / len(tokens)\n",
    "        feature_vector += list(vecs)\n",
    "    \n",
    "    # Punctuation counts marks\n",
    "    if \"punctuation\" in features:\n",
    "        puncts = []\n",
    "        puncts.append(sent.count(\".\"))\n",
    "        puncts.append(sent.count(\"...\"))\n",
    "        puncts.append(sent.count(\",\"))\n",
    "        puncts.append(sent.count(\"(\"))\n",
    "        puncts.append(sent.count(\")\"))\n",
    "        puncts.append(sent.count(\":\"))\n",
    "        puncts.append(sent.count(\";\"))\n",
    "        puncts.append(sent.count('\"'))\n",
    "        feature_vector += puncts\n",
    "        \n",
    "    # Hashtag count\n",
    "    if \"hashtag\" in features:\n",
    "        feature_vector += [sent.count(\"#\")]\n",
    "        \n",
    "    # Is Retweet\n",
    "    if \"retweet\" in features:\n",
    "        retweet = []\n",
    "        if \"b'RT \" in sent or \"bRT \" in sent or \" rt \" in sent:\n",
    "            retweet.append(1)\n",
    "        else:\n",
    "            retweet.append(0)\n",
    "        feature_vector += retweet\n",
    "        \n",
    "    # ratio of words in all caps\n",
    "    if \"caps\" in features:\n",
    "        filterwords = [token for token in caps_tokens.findall(sent) if token.isalpha() and len(token) > 1]\n",
    "        caps = [word for word in filterwords if word.isupper()]\n",
    "        if len(filterwords) > 0:\n",
    "            ratio = len(caps) / len(filterwords)\n",
    "        else:\n",
    "            ratio = 0\n",
    "        feature_vector += [ratio]\n",
    "    \n",
    "    # Emoji count\n",
    "    if \"emoji\" in features:\n",
    "        emojis = 0\n",
    "        for char in sent:\n",
    "            if emoji.is_emoji(char):\n",
    "                emojis += 1\n",
    "        feature_vector += [emojis]\n",
    "        \n",
    "    \n",
    "    return np.array(feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9940030",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(df['class'] == \"implicit_hate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "989ec7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started testing at 17:15.28\n",
      "finished epoch 0 at 17:40.13\n",
      "finished epoch 1 at 18:13.57\n",
      "finished epoch 5 at 18:50.48\n",
      "finished epoch 25 at 19:27.01\n",
      "finished epoch 150 at 19:47.32\n",
      "finished epoch 625 at 20:06.28\n",
      "finished epoch 3125 at 20:24.53\n"
     ]
    }
   ],
   "source": [
    "all_epochs = [1, 5, 25, 150, 625, 3125]\n",
    "stats = pd.DataFrame(index=pd.Index([0] + all_epochs, name=\"epochs\"), columns=[\"accuracy\", \"precision\", \"recall\"])\n",
    "print(\"Started testing at\", time(\"%H:%M.%S\"))\n",
    "for epoch in [0] + all_epochs:\n",
    "    epoch_model = fasttext.FastText.load(get_tmpfile(\"C:/Users/flohk/OneDrive/Uni/Projektseminar/data/fasttext-finetune/model\" + str(epoch) + \".model\")).wv if epoch != 0 else load_standard_model().wv\n",
    "    X = StandardScaler().fit_transform(np.array([vectorize(tweet, features=[\"fasttext\", \"punctuation\", \"hashtag\", \"retweet\", \"caps\", \"emoji\"], model=epoch_model) for tweet in df[\"post\"]]))   \n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "    acc, prec, rec = 0, 0, 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        svm = SVC(kernel=\"rbf\", C=100, gamma=0.005)\n",
    "        svm.fit(X_train, y_train)\n",
    "        test_labels = svm.predict(X_test)\n",
    "        acc += accuracy_score(y_true=y_test, y_pred=test_labels)\n",
    "        prec += precision_score(y_true=y_test, y_pred=test_labels, zero_division = 0)\n",
    "        rec += recall_score(y_true=y_test, y_pred=test_labels)\n",
    "    stats[\"accuracy\"][epoch] = acc/5\n",
    "    stats[\"precision\"][epoch] = prec/5\n",
    "    stats[\"recall\"][epoch] = rec/5\n",
    "    print(\"finished epoch\", epoch, \"at\", time(\"%H:%M.%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9a4ddca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epochs</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.73145</td>\n",
       "      <td>0.632315</td>\n",
       "      <td>0.546253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.711785</td>\n",
       "      <td>0.593706</td>\n",
       "      <td>0.546138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.712177</td>\n",
       "      <td>0.591931</td>\n",
       "      <td>0.558137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.714433</td>\n",
       "      <td>0.595154</td>\n",
       "      <td>0.562163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.737286</td>\n",
       "      <td>0.648467</td>\n",
       "      <td>0.536891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>0.742582</td>\n",
       "      <td>0.668834</td>\n",
       "      <td>0.516974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3125</th>\n",
       "      <td>0.738806</td>\n",
       "      <td>0.666247</td>\n",
       "      <td>0.50023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        accuracy precision    recall\n",
       "epochs                              \n",
       "0        0.73145  0.632315  0.546253\n",
       "1       0.711785  0.593706  0.546138\n",
       "5       0.712177  0.591931  0.558137\n",
       "25      0.714433  0.595154  0.562163\n",
       "150     0.737286  0.648467  0.536891\n",
       "625     0.742582  0.668834  0.516974\n",
       "3125    0.738806  0.666247   0.50023"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229045e7",
   "metadata": {},
   "source": [
    "Finetuning does not seem to have a significant effect on the semantic strength of the embeddings for the task at hand. However, we will keep the 150 epoch set for now. The others will be deleted, because the each take up almost 10 GB of disk space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
