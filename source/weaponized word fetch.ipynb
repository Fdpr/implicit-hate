{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26a6d1b4",
   "metadata": {},
   "source": [
    "(Because the weaponized word lexicon's license does not allow for redistribution, query results will be deleted and relevant log outputs from the notebook will not be shown) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8eaed90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pickle\n",
    "from itertools import chain\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a00ae9",
   "metadata": {},
   "source": [
    "## The weaponized word lexicon\n",
    "The weaponized word (https://weaponizedword.org) provides lexicons of hate words for different use cases. Although all of them seem interesting, we are only going to look at the discriminatory words here. First, all relevant terms need to be fetched from the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8e6fd28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth(key):\n",
    "    response = requests.post(\"https://api.weaponizedword.org/lexicons/1-0/authenticate\", {\"api_key\": key})\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a96ffbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discriminatory(token):\n",
    "    last_status = 200\n",
    "    num = 1\n",
    "    result = []\n",
    "    while last_status == 200 and num <= 16:\n",
    "        response = requests.post(\"https://api.weaponizedword.org/lexicons/1-0/get_discriminatory\", {\"token\": token, \"page\": num, \"language_id\": \"eng\"})\n",
    "        last_status = response.status_code\n",
    "        if last_status == 200:\n",
    "            result.append(response.json())\n",
    "        else:\n",
    "            break\n",
    "        num += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef37a658",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = auth(\"MY API KEY\")[\"token\"] \n",
    "discriminatory = get_discriminatory(\"token\")\n",
    "file = open(\"data/hate_words/discriminatory.pkl\", \"wb\")\n",
    "pickle.dump(discriminatory, file)\n",
    "file.close()\n",
    "discriminatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27deb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = list(chain.from_iterable([item[\"result\"] for item in discriminatory[:16]]))\n",
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e1af6c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1576"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d0c827",
   "metadata": {},
   "source": [
    "Now we over 1000 terms. However, because the goal is to possibly extract a hatefulness component from these terms, we should disregard any that are not strongly hateful. The idea is that the embeddings probably cannot pick up some of the subtleties and thus would lead to noise if we use too many too weak terms. Weaponized word annotates a term's offensiveness and thus we can filter out any words that are not strongly offensive. Also, the items may include archaic terms which are unwanted as well.\n",
    "\n",
    "Or rather, this is what we would want ideally. However, after applying these heavy filters, there are only about 30 terms left. We need more for a strong categorization, so every term and term variation will be accepted into the hate term set. For now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "4679125e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1576"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extremes = [item for item in items]\n",
    "# extremes = [item for item in items if item[\"offensiveness\"] == \"Extremely offensive\" or item[\"offensiveness\"] == \"Very offensive\" or item[\"offensiveness\"] == \"Significantly offensive\" or item[\"offensiveness\"] == \"Moderately offensive\"]\n",
    "# extremes = [item for item in extremes if item[\"is_archaic\"] == \"N\"]\n",
    "# extremes = [item for item in extremes if item[\"variant_of_id\"] == None]\n",
    "# extremes = [item for item in extremes if item[\"plural_of_id\"] == None]\n",
    "len(extremes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce311dc",
   "metadata": {},
   "source": [
    "Now, the terms can be grouped by target group. There are 7 supercategories:\n",
    "- nationality\n",
    "- ethnicity\n",
    "- religion\n",
    "- gender\n",
    "- orientation\n",
    "- disability\n",
    "- class\n",
    "\n",
    "All but the last two contain further subcategories. We can thus group the terms by target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "71e21a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "nationality = defaultdict(lambda: [])\n",
    "ethnicity = defaultdict(lambda: [])\n",
    "religion = defaultdict(lambda: [])\n",
    "gender = defaultdict(lambda: [])\n",
    "orientation = defaultdict(lambda: [])\n",
    "disability = []\n",
    "class_group = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "0afcbcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for term in extremes:\n",
    "    if term[\"is_about_nationality\"] == \"Y\" and term[\"nationalities\"] is not None:\n",
    "        for national in term[\"nationalities\"]:\n",
    "            nationality[national].append(term[\"term\"])\n",
    "    if term[\"is_about_ethnicity\"] == \"Y\" and term[\"athnicities\"] is not None:\n",
    "        for ethnic in term[\"athnicities\"]:\n",
    "            ethnicity[ethnic].append(term[\"term\"])\n",
    "    if term[\"is_about_religion\"] == \"Y\" and term[\"religions\"] is not None:\n",
    "        for relig in term[\"religions\"]:\n",
    "            religion[relig].append(term[\"term\"])\n",
    "    if term[\"is_about_gender\"] == \"Y\" and term[\"genders\"] is not None:\n",
    "        for gen in term[\"genders\"]:\n",
    "            gender[gen].append(term[\"term\"])\n",
    "    if term[\"is_about_orientation\"] == \"Y\" and term[\"orientations\"] is not None:\n",
    "        for orient in term[\"orientations\"]:\n",
    "            orientation[orient].append(term[\"term\"])\n",
    "    if term[\"is_about_disability\"] == \"Y\":\n",
    "        disability.append(term[\"term\"])\n",
    "    if term[\"is_about_class\"] == \"Y\":\n",
    "        class_group.append(term[\"term\"])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "64fb750f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nationalities: ['TR', 'PH', 'IN', 'DE', 'IE', 'GB', 'PS', 'PF', 'ZA', 'JP', 'AU', 'LT', 'LB', 'AL', 'US', 'NL', 'MX', 'NZ', 'IT', 'CN', 'CA', 'PK', 'CU', 'VN', 'AR', 'MK', 'CZ', 'UA', 'SO', 'HU', 'PL', 'WS', 'EG', 'DO']\n",
      "Ethnicities: ['African', 'African American', 'Arabs', 'European', 'German', 'Jews', 'Irish', 'Italian', 'Bihari', 'Japanese', 'Aboriginal', 'Albanian', 'Asian', 'English', 'Hispanic', 'Chinese', 'Dinka', 'Korean', 'Pacific Islander', 'Romani', 'Egyptian', 'Portuguese', 'Sardinian', 'Spaniard', 'Czech', 'Ukrainian', 'French', 'Hungarian', 'Inuit', 'Vietnamese', 'Hawaiian', 'Samoan', 'Pakistani', 'India', 'Tamil']\n",
      "Genders: ['female', 'male']\n",
      "Orientations: ['homosexual']\n"
     ]
    }
   ],
   "source": [
    "print(\"Nationalities:\", list(nationality.keys()))\n",
    "print(\"Ethnicities:\", list(ethnicity.keys()))\n",
    "print(\"Genders:\", list(gender.keys()))\n",
    "print(\"Orientations:\", list(orientation.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b09ae62",
   "metadata": {},
   "source": [
    "There are a couple groups now. However, these terms still need to be hand-filtered. Because semantic embeddings don't capture context very well, we should disregard terms that are made out of multiple word compounds and those who have a very common non-hate meaning. This should make the component filtering more robust. Also, we will disregard categories with very few hate words associated with them. This will lead to a more robust result as well, although it will have as a consequences that not all target groups will be captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a660c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_terms = []\n",
    "for key in nationality.keys():\n",
    "    nats = set([item.lower() for item in nationality[key] if not \" \" in item])\n",
    "    if (len(nats) >= 3):\n",
    "        print(key, nats)\n",
    "        all_terms += nats\n",
    "for key in ethnicity.keys():\n",
    "    ets = set([item.lower() for item in ethnicity[key] if not \" \" in item])\n",
    "    if (len(ets) >= 3):\n",
    "        print(key, ets)\n",
    "        all_terms += ets\n",
    "for key in gender.keys():\n",
    "    gen = set([item.lower() for item in gender[key] if not \" \" in item])\n",
    "    if (len(gen) >= 3):\n",
    "        print(key, gen)\n",
    "        all_terms += gen\n",
    "for key in orientation.keys():\n",
    "    ors = set([item.lower() for item in orientation[key] if not \" \" in item])\n",
    "    if (len(ors) >= 3):\n",
    "        print(key, ors)\n",
    "        all_terms += ors\n",
    "for key in religion.keys():\n",
    "    rel = set([item.lower() for item in religion[key] if not \" \" in item])\n",
    "    if (len(rel) >= 3):\n",
    "        print(key, rel)\n",
    "        all_terms += rel\n",
    "dis = set([item.lower() for item in disability if not \" \" in item])\n",
    "if (len(dis) >= 3):\n",
    "    print(\"disability\", dis)\n",
    "    all_terms += dis\n",
    "cla = set([item.lower() for item in class_group if not \" \" in item])\n",
    "if (len(cla) >= 3):\n",
    "    print(\"class\", cla)\n",
    "    all_terms += dis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410cb040",
   "metadata": {},
   "source": [
    "Adding the singular terms from davidsons n-gram dictionary (https://github.com/t-davidson/hate-speech-and-offensive-language/blob/master/lexicons/refined_ngram_dict.csv) to the list to compile the final term set. Because of the metric we are using here, we can simply combine all the terms into one big set. However, a more refined approach might include seperate metrics for hate based on group identity. Since we might come back to that later, it is useful to have the terms separated by group as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "cd9b7762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A BIG LIST OF BAD WORDS]\n"
     ]
    }
   ],
   "source": [
    "hate_terms = list(set([term for term in all_terms if not \"-\" in term]))\n",
    "hate_terms += [\"chink\", \"dyke\", \"faggot\", \"nigger\", \"spic\"]\n",
    "print(list(set(hate_terms)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e290feb3",
   "metadata": {},
   "source": [
    "Having compiled a set of terms, we can now train the similarity classifier used in the Relative sentiment Bias metric. Using this classifier, we will be able to assign a score to each word or phrase that measures its propensity for hatefulness (or, more specifically: its propensity to be similar to a derogatory term from the generated list). RNSB scoring is performed by comparing against two sets, usually a set of positive vs negative sentiment words. We will swap the negative sentiment word list for the hate term set and work under the assumption that *hate vs. positive* instead of *negative vs. positive* will still work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "4f9c3476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wefe.datasets import load_bingliu\n",
    "from wefe.metrics import RNSB\n",
    "from gensim.models import fasttext\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "23e4e63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_terms = load_bingliu()['positive_words']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7ef1e5",
   "metadata": {},
   "source": [
    "Since we want to use the basic as well as the finetuned word embeddings, two classifiers will be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "1617011b",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_embeddings = fasttext.load_facebook_vectors(datapath(\"C:/Users/flohk/OneDrive/Uni/Projektseminar/data/crawl-300d-2M-subword/crawl-300d-2M-subword.bin\"))\n",
    "finetuned_embeddings = fasttext.FastText.load(datapath(\"C:/Users/flohk/OneDrive/Uni/Projektseminar/data/fasttext-finetune/model150.model\")).wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "e0793c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_vectors = [{word: basic_embeddings[word] for word in positive_terms}, {word: basic_embeddings[word] for word in hate_terms}]\n",
    "finetuned_vectors = [{word: finetuned_embeddings[word] for word in positive_terms}, {word: finetuned_embeddings[word] for word in hate_terms}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "d954c871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic embeddings Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       1.00      0.89      0.94       109\n",
      "         1.0       0.97      1.00      0.99       402\n",
      "\n",
      "    accuracy                           0.98       511\n",
      "   macro avg       0.99      0.94      0.96       511\n",
      "weighted avg       0.98      0.98      0.98       511\n",
      "\n",
      "Finetuned embeddings Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.82      0.85      0.84       109\n",
      "         1.0       0.96      0.95      0.96       402\n",
      "\n",
      "    accuracy                           0.93       511\n",
      "   macro avg       0.89      0.90      0.90       511\n",
      "weighted avg       0.93      0.93      0.93       511\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Basic embeddings \", end=\"\")\n",
    "basic_classifier = RNSB()._train_classifier(attribute_embeddings_dict=basic_vectors, print_model_evaluation=True)\n",
    "print(\"Finetuned embeddings \", end=\"\")\n",
    "finetuned_classifier = RNSB()._train_classifier(attribute_embeddings_dict=finetuned_vectors, print_model_evaluation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55df5425",
   "metadata": {},
   "source": [
    "Ouch! The finetuned model really looks like it overfitted quite a bit when compared to the metrics of the basic embeddings. However, we can still continue with it. For now, our work here is done. The classifier scores can now be used as features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "b0eab7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"data/RNSB_classifiers/basic_classifier.pkl\", \"wb\")\n",
    "pickle.dump(basic_classifier, file)\n",
    "file.close()\n",
    "file = open(\"data/RNSB_classifiers/finetuned_classifier.pkl\", \"wb\")\n",
    "pickle.dump(finetuned_classifier, file)\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
